{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc7af728",
   "metadata": {},
   "source": [
    "# Import des bibliothèques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407312a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "import cv2\n",
    "import open_clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcbab35",
   "metadata": {},
   "source": [
    "# Installation de Detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe0442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36e17099",
   "metadata": {},
   "source": [
    "# Construction de Runa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59377261",
   "metadata": {},
   "source": [
    "### Import du faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe82c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\n",
    "    model_zoo.get_config_file(\n",
    "        \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
    "    )\n",
    ")\n",
    "\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
    "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
    ")\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324371f2",
   "metadata": {},
   "source": [
    "### Fonction pour blur et crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2205e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_crop_and_blurv2(image, predictor, blur_ksize=51):\n",
    "    \"\"\"\n",
    "    image : numpy array BGR (cv2.imread)\n",
    "    predictor : Detectron2 DefaultPredictor\n",
    "    blur_ksize : taille du kernel de flou (impair)\n",
    "\n",
    "    retourne :\n",
    "        results : liste de dict :\n",
    "            {\n",
    "                \"box\": [x1, y1, x2, y2],\n",
    "                \"crop\": image cropée,\n",
    "                \"blurred\": image globale avec bbox floutée\n",
    "            }\n",
    "    \"\"\"\n",
    "\n",
    "    outputs = predictor(image)\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "\n",
    "    if len(instances) == 0:\n",
    "        return []\n",
    "\n",
    "    boxes = instances.pred_boxes.tensor.numpy().astype(int)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "\n",
    "        # ---- Crop régional (I(r)) ----\n",
    "        crop = image[y1:y2, x1:x2].copy()\n",
    "\n",
    "        # ---- Image globale avec bbox floutée (I(g)) ----\n",
    "        blurred_img = image.copy()\n",
    "\n",
    "        roi = blurred_img[y1:y2, x1:x2]\n",
    "        roi_blur = cv2.GaussianBlur(roi, (blur_ksize, blur_ksize), 0)\n",
    "\n",
    "        blurred_img[y1:y2, x1:x2] = roi_blur\n",
    "\n",
    "        results.append({\n",
    "            \"box\": [x1, y1, x2, y2],\n",
    "            \"crop\": crop,\n",
    "            \"blurred\": blurred_img\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e207c23",
   "metadata": {},
   "source": [
    "### Mise en place de CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf128e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model, preprocess, tokenizer = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-16\",\n",
    "    pretrained=\"openai\"\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_clip(image_bgr):\n",
    "    \"\"\"\n",
    "    image_bgr : image OpenCV\n",
    "    retourne : embedding torch (1, D)\n",
    "    \"\"\"\n",
    "\n",
    "    # BGR → RGB\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # numpy → PIL\n",
    "    pil_img = Image.fromarray(image_rgb)\n",
    "\n",
    "    # preprocessing CLIP\n",
    "    image_input = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = model.encode_image(image_input)\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_runa_batch(results):\n",
    "\n",
    "    crop_embeddings = []\n",
    "    global_embeddings = []\n",
    "\n",
    "    for r in results:\n",
    "        emb_crop = encode_image_clip(r[\"crop\"])\n",
    "        emb_global = encode_image_clip(r[\"blurred\"])\n",
    "\n",
    "        crop_embeddings.append(emb_crop)\n",
    "        global_embeddings.append(emb_global)\n",
    "\n",
    "    return crop_embeddings, global_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c605fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embedding(emb):\n",
    "    return emb / emb.norm(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b7d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_embeddings(crop_emb, global_emb, lam=0.5):\n",
    "    fused = lam * crop_emb + (1 - lam) * global_emb\n",
    "    fused = fused / fused.norm(dim=-1, keepdim=True)\n",
    "    return fused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55bd7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# récupérer le tokenizer correct\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-16\")\n",
    "\n",
    "voc_labels1 = [\n",
    "    \"a photo of a aeroplane\",\"a photo of a bicycle\",\"a photo of a bird\",\n",
    "    \"a photo of a boat\",\"a photo of a bottle\",\"a photo of a bus\",\n",
    "    \"a photo of a car\",\"a photo of a cat\",\"a photo of a chair\",\n",
    "    \"a photo of a cow\",\"a photo of a dining table\",\"a photo of a dog\",\n",
    "    \"a photo of a horse\",\"a photo of a motorbike\",\"a photo of a person\",\n",
    "    \"a photo of a potted plant\",\"a photo of a sheep\",\"a photo of a sofa\",\n",
    "    \"a photo of a train\",\"a photo of a tv monitor\"\n",
    "]\n",
    "\n",
    "text_tokens = tokenizer(voc_labels1)\n",
    "text_tokens = text_tokens.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model.encode_text(text_tokens)\n",
    "\n",
    "text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(text_embeddings.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runa_score(fused_emb, text_embeddings):\n",
    "    similarities = fused_emb @ text_embeddings.T   # cosine similarity car embeddings normalisés\n",
    "    max_sim = similarities.max().item()\n",
    "    score = -max_sim\n",
    "    return score, similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runa_score_image(image, predictor, text_embeddings):\n",
    "\n",
    "    # 1. Detectron2 + crop + blur\n",
    "    results = detect_crop_and_blurv2(image, predictor)\n",
    "\n",
    "    if len(results) == 0:\n",
    "        return None   # aucune bbox détectée\n",
    "\n",
    "    # 2. CLIP embeddings\n",
    "    crop_emb, global_emb = encode_runa_batch(results)\n",
    "\n",
    "    # 3. Score RUNA pour chaque bbox\n",
    "    scores = []\n",
    "    for i in range(len(crop_emb)):\n",
    "        fused = fuse_embeddings(crop_emb[i], global_emb[i])\n",
    "        score, _ = runa_score(fused, text_embeddings)\n",
    "        scores.append(score)\n",
    "\n",
    "    # 4. Score image = meilleur objet\n",
    "    return min(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(\n",
    "    img_paths,\n",
    "    predictor,\n",
    "    text_embeddings,\n",
    "    save_path,\n",
    "    save_every=50,\n",
    "    desc=\"Processing images\"\n",
    "):\n",
    "    scores = []\n",
    "    processed = 0\n",
    "\n",
    "    for img_path in tqdm(img_paths, desc=desc):\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        score = runa_score_image(image, predictor, text_embeddings)\n",
    "\n",
    "        if score is not None:\n",
    "            scores.append(score)\n",
    "            processed += 1\n",
    "\n",
    "            # sauvegarde progressive\n",
    "            if processed % save_every == 0:\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    pickle.dump(scores, f)\n",
    "\n",
    "    # sauvegarde finale\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(scores, f)\n",
    "\n",
    "    print(f\"\\n{desc}\")\n",
    "    print(\"Nombre d'images scorées:\", len(scores))\n",
    "    if len(scores) > 0:\n",
    "        print(\"Min/Max scores:\", min(scores), max(scores))\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540bd3b3",
   "metadata": {},
   "source": [
    "### IID : Pascal VOC2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e215f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_root = \"VOC/VOCtest_train_06-Nov-2007/VOCdevkit/VOC2007\"\n",
    "img_dir = os.path.join(voc_root, \"JPEGImages\")\n",
    "split_file = os.path.join(voc_root, \"ImageSets\", \"Main\", \"test.txt\")\n",
    "\n",
    "with open(split_file, \"r\") as f:\n",
    "    test_ids = [line.strip() for line in f]\n",
    "\n",
    "img_paths_voc = [os.path.join(img_dir, f\"{img_id}.jpg\") for img_id in test_ids]\n",
    "print(\"Nb images test:\", len(img_paths_voc))\n",
    "print(\"Exemple:\", img_paths_voc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e70c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_voc = compute_scores(\n",
    "    img_paths=img_paths_voc,\n",
    "    predictor=predictor,\n",
    "    text_embeddings=text_embeddings,\n",
    "    save_path=\"scores_voc2007_test.pkl\",\n",
    "    save_every=50,\n",
    "    desc=\"VOC 2007 Test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4245f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scores_voc2007_test.pkl\", \"rb\") as f:\n",
    "    scores_voc = pickle.load(f)\n",
    "\n",
    "print(type(scores_voc))\n",
    "print(len(scores_voc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b943e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(scores_voc, bins=50)\n",
    "plt.title(\"Distribution des scores VOC\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Nombre d'images\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ba307",
   "metadata": {},
   "source": [
    "### OOD: MS COCO 2014 non filtré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dir = \"coco/images/val2014\"\n",
    "img_paths_coco = [\n",
    "    os.path.join(coco_dir, f)\n",
    "    for f in os.listdir(coco_dir)\n",
    "    if f.endswith(\".jpg\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f43fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_coco = compute_scores(\n",
    "    img_paths=img_paths_coco,\n",
    "    predictor=predictor,\n",
    "    text_embeddings=text_embeddings,\n",
    "    save_path=\"scores_coco2014_val.pkl\",\n",
    "    save_every=50,\n",
    "    desc=\"COCO 2014 Val\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scores_coco2014_val.pkl\", \"rb\") as f:\n",
    "    scores_coco = pickle.load(f)\n",
    "\n",
    "print(len(scores_coco))\n",
    "print(scores_coco[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b94d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ood(scores_id, scores_ood, id_name=\"ID\", ood_name=\"OOD\"):\n",
    "    scores_id = list(scores_id)\n",
    "    scores_ood = list(scores_ood)\n",
    "\n",
    "    print(f\"{id_name} mean:\", np.mean(scores_id))\n",
    "    print(f\"{ood_name} mean:\", np.mean(scores_ood))\n",
    "\n",
    "    y_true = [0] * len(scores_id) + [1] * len(scores_ood)\n",
    "    y_scores = scores_id + scores_ood\n",
    "\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    print(\"AUROC:\", auc)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "\n",
    "    idx = np.argmin(np.abs(tpr - 0.95))\n",
    "    fpr95 = fpr[idx]\n",
    "\n",
    "    print(\"FPR95:\", fpr95)\n",
    "\n",
    "    return {\n",
    "        \"id_mean\": np.mean(scores_id),\n",
    "        \"ood_mean\": np.mean(scores_ood),\n",
    "        \"auroc\": auc,\n",
    "        \"fpr95\": fpr95,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c937086",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_voc_coco = evaluate_ood(\n",
    "    scores_voc,\n",
    "    scores_coco,\n",
    "    id_name=\"VOC\",\n",
    "    ood_name=\"COCO\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fdbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_distributions(scores_id, scores_ood,\n",
    "                             id_name=\"ID\", ood_name=\"OOD\",\n",
    "                             bins=50):\n",
    "    plt.hist(scores_id, bins=bins, alpha=0.5, label=f\"{id_name} (ID)\")\n",
    "    plt.hist(scores_ood, bins=bins, alpha=0.5, label=f\"{ood_name} (OOD)\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Score distributions\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a592a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_distributions(scores_voc, scores_coco, \"VOC\", \"COCO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdef717",
   "metadata": {},
   "source": [
    "### MS COCO 2014 filtré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b6c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_file = \"coco/annotations/instances_val2014.json\"\n",
    "coco = COCO(ann_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33caa195",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = coco.loadCats(coco.getCatIds())\n",
    "cat_names = [c[\"name\"] for c in cats]\n",
    "print(cat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec7a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_labels_coco = {\n",
    "\"airplane\",\"bicycle\",\"bird\",\"boat\",\"bottle\",\"bus\",\"car\",\"cat\",\n",
    "\"chair\",\"cow\",\"dining table\",\"dog\",\"horse\",\"motorcycle\",\n",
    "\"person\",\"potted plant\",\"sheep\",\"couch\",\"train\",\"tv\"\n",
    "}\n",
    "\n",
    "cats = coco.loadCats(coco.getCatIds())\n",
    "cat_ids = [c[\"id\"] for c in cats if c[\"name\"] in voc_labels_coco]\n",
    "\n",
    "print(\"Cat IDs:\", cat_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b422094",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids_with_voc = set()\n",
    "\n",
    "for cat_id in cat_ids:\n",
    "    ann_ids = coco.getAnnIds(catIds=[cat_id])\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    for ann in anns:\n",
    "        img_ids_with_voc.add(ann[\"image_id\"])\n",
    "\n",
    "print(\"Images contenant classes VOC:\", len(img_ids_with_voc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb146c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_ids = set(coco.getImgIds())\n",
    "ood_img_ids = list(all_img_ids - img_ids_with_voc)\n",
    "\n",
    "print(\"Images OOD:\", len(ood_img_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths_ood = [\n",
    "    os.path.join(coco_dir, coco.loadImgs(img_id)[0][\"file_name\"])\n",
    "    for img_id in ood_img_ids\n",
    "]\n",
    "\n",
    "print(\"Exemple:\", img_paths_ood[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba1aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_coco_filtered = compute_scores(\n",
    "    img_paths=img_paths_ood,\n",
    "    predictor=predictor,\n",
    "    text_embeddings=text_embeddings,\n",
    "    save_path=\"scores_coco_filtered.pkl\",\n",
    "    save_every=100,\n",
    "    desc=\"COCO Filtered OOD\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ffb336",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scores_coco_filtered.pkl\", \"rb\") as f:\n",
    "    scores_coco_filtered = pickle.load(f)\n",
    "\n",
    "print(type(scores_coco_filtered))\n",
    "print(len(scores_coco_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e79c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_voc_coco_filtered = evaluate_ood(\n",
    "    scores_voc,\n",
    "    scores_coco_filtered,\n",
    "    id_name=\"VOC\",\n",
    "    ood_name=\"COCO filtered\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b307089",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_distributions(scores_voc, scores_coco_filtered,\n",
    "                         \"VOC\", \"COCO filtered\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
